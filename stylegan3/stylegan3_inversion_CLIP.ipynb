{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aea19808-8f02-4209-8d7a-195ea4924444",
   "metadata": {},
   "source": [
    "On OOD, load `torch-gpu-stylegan` kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e11d1f-b22d-4f49-a0f2-33bd64e320d0",
   "metadata": {},
   "source": [
    "Winter Break 2022 work. \n",
    "\n",
    "Run on jupyterhub-west.nrp-nautilus.io with a RTX A100 GPU.\n",
    "\n",
    "Adapted from https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2Binversion%2BCLIP.ipynb#scrollTo=5K38uyFrv5wo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e3ab2-5e84-4e2f-aa50-5aee170ba6b3",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "One time install of libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459f1185-d8e7-4251-9e47-5f95e8c0903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/NVlabs/stylegan3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ea739-acdf-4f07-a002-9ca45183891d",
   "metadata": {},
   "source": [
    "# 1. Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9db054a-c99c-46f5-ba7f-cae04be130ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append('./CLIP')\n",
    "sys.path.append('./stylegan3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca6ac9a-e670-4620-ae6a-5ae9389c7542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os, time\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import clip\n",
    "import copy\n",
    "import imageio\n",
    "import unicodedata\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from IPython.display import display\n",
    "from einops import rearrange\n",
    "# from google.colab import files\n",
    "from time import perf_counter\n",
    "from stylegan3.dnnlib.util import open_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4e49f2-9cd8-4dd4-b359-8f4c3b24f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Load VGG16 feature detector.\n",
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "with open_url(url) as f:\n",
    "    vgg16 = torch.jit.load(f).eval().to(device)\n",
    "print('Using device:', device, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b755d9-96b8-4e9c-88f1-237c6cbda633",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245eb5f9-53a9-4832-816c-9f6262166ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "def fetch_model(url_or_path):\n",
    "    if \"drive.google\" in url_or_path:\n",
    "      if \"18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj\" in url_or_path: \n",
    "        basename = \"wikiart-1024-stylegan3-t-17.2Mimg.pkl\"\n",
    "      elif \"14UGDDOusZ9TMb-pOrF0PAjMGVWLSAii1\" in url_or_path:\n",
    "        basename = \"lhq-256-stylegan3-t-25Mimg.pkl\"\n",
    "    else:\n",
    "        basename = os.path.basename(url_or_path)\n",
    "    if os.path.exists(basename):\n",
    "        return basename\n",
    "    else:\n",
    "        if \"drive.google\" not in url_or_path:\n",
    "          !wget -c '{url_or_path}'\n",
    "        else:\n",
    "          path_id = url_or_path.split(\"id=\")[-1]\n",
    "          !gdown --id '{path_id}'\n",
    "        return basename\n",
    "\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "def norm1(prompt):\n",
    "    \"Normalize to the unit sphere.\"\n",
    "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "class MakeCutouts(torch.nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
    "\n",
    "def embed_image(image):\n",
    "  n = image.shape[0]\n",
    "  cutouts = make_cutouts(image)\n",
    "  embeds = clip_model.embed_cutout(cutouts)\n",
    "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
    "  return embeds\n",
    "\n",
    "def embed_url(url):\n",
    "  image = Image.open(fetch(url)).convert('RGB')\n",
    "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
    "\n",
    "class CLIP(object):\n",
    "  def __init__(self):\n",
    "    clip_model = \"ViT-B/32\"\n",
    "    self.model, _ = clip.load(clip_model)\n",
    "    self.model = self.model.requires_grad_(False)\n",
    "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def embed_text(self, prompt):\n",
    "      \"Normalized clip text embedding.\"\n",
    "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
    "\n",
    "  def embed_cutout(self, image):\n",
    "      \"Normalized clip image embedding.\"\n",
    "      return norm1(self.model.encode_image(self.normalize(image)))\n",
    "  \n",
    "clip_model = CLIP()\n",
    "\n",
    "# Projector\n",
    "\n",
    "def project(\n",
    "    G,\n",
    "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
    "    *,\n",
    "    num_steps                  = 1000,\n",
    "    w_avg_samples              = -1,\n",
    "    initial_learning_rate      = 0.1,\n",
    "    initial_noise_factor       = 0.05,\n",
    "    lr_rampdown_length         = 0.25,\n",
    "    lr_rampup_length           = 0.05,\n",
    "    noise_ramp_length          = 0.75,\n",
    "    regularize_noise_weight    = 1e5,\n",
    "    verbose                    = False,\n",
    "    device: torch.device\n",
    "):\n",
    "\n",
    "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
    "\n",
    "    def logprint(*args):\n",
    "        if verbose:\n",
    "            print(*args)\n",
    "\n",
    "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "    # Compute w stats.\n",
    "    if w_avg_samples > 0:\n",
    "      logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
    "      z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
    "    else:\n",
    "      seed = np.random.randint(0, 2**32 - 1)\n",
    "      z_samples = np.random.RandomState(seed).randn(1, G.z_dim)\n",
    "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
    "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
    "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
    "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
    "\n",
    "    # Setup noise inputs.\n",
    "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
    "\n",
    "    # Features for target image.\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
    "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
    "\n",
    "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
    "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
    "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
    "\n",
    "    # Init noise.\n",
    "    for buf in noise_bufs.values():\n",
    "        buf[:] = torch.randn_like(buf)\n",
    "        buf.requires_grad = True\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Learning rate schedule.\n",
    "        t = step / num_steps\n",
    "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
    "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "        lr = initial_learning_rate * lr_ramp\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Synth images from opt_w.\n",
    "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
    "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
    "        synth_images = G.synthesis(ws, noise_mode='const')\n",
    "\n",
    "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "        synth_images = (synth_images + 1) * (255/2)\n",
    "        if synth_images.shape[2] > 256:\n",
    "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "        # Features for synth images.\n",
    "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
    "        dist = (target_features - synth_features).square().sum()\n",
    "\n",
    "        # Noise regularization.\n",
    "        reg_loss = 0.0\n",
    "        for v in noise_bufs.values():\n",
    "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
    "            while True:\n",
    "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
    "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
    "                if noise.shape[2] <= 8:\n",
    "                    break\n",
    "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
    "        loss = dist + reg_loss * regularize_noise_weight\n",
    "\n",
    "        # Step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
    "\n",
    "        # Save projected W for each optimization step.\n",
    "        w_out[step] = w_opt.detach()[0]\n",
    "\n",
    "        # Normalize noise.\n",
    "        with torch.no_grad():\n",
    "            for buf in noise_bufs.values():\n",
    "                buf -= buf.mean()\n",
    "                buf *= buf.square().mean().rsqrt()\n",
    "\n",
    "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
    "\n",
    "def get_perceptual_loss(synth_image, target_features):\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    if synth_image.shape[2] > 256:\n",
    "        synth_image = F.interpolate(synth_image, size=(256, 256), mode='area')\n",
    "\n",
    "    # Features for synth images.\n",
    "    synth_features = vgg16(synth_image, resize_images=False, return_lpips=True)\n",
    "    return (target_features - synth_features).square().sum()\n",
    "\n",
    "def get_target_features(target):\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
    "    return vgg16(target_images, resize_images=False, return_lpips=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1a676-993a-45dd-830b-4174d3854f43",
   "metadata": {},
   "source": [
    "Model selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e53819-ed08-4124-924d-022c9de8d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/utils/cpp_extension.py:266: UserWarning: \n",
      "\n",
      "                               !! WARNING !!\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Your compiler (c++) is not compatible with the compiler Pytorch was\n",
      "built with for this platform, which is g++ on linux. Please\n",
      "use g++ to to compile your extension. Alternatively, you may\n",
      "compile PyTorch from source using c++, and then you can also use\n",
      "c++ to compile your extension.\n",
      "\n",
      "See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help\n",
      "with compiling PyTorch from source.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "                              !! WARNING !!\n",
      "\n",
      "  warnings.warn(WRONG_COMPILER_WARNING.format(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'bias_act_plugin': [1/3] /work/emar349/shared/envs/torch-gpu-stylegan/bin/nvcc -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/TH -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/THC -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/twomeylab/rtwomey/.cache/torch_extensions/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-tesla-v100-pcie-32gb/bias_act.cu -o bias_act.cuda.o \n[2/3] c++ -MMD -MF bias_act.o.d -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/TH -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/THC -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /home/twomeylab/rtwomey/.cache/torch_extensions/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-tesla-v100-pcie-32gb/bias_act.cpp -o bias_act.o \n[3/3] c++ bias_act.o bias_act.cuda.o -shared -L/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/work/emar349/shared/envs/torch-gpu-stylegan/lib64 -lcudart -o bias_act_plugin.so\nFAILED: bias_act_plugin.so \nc++ bias_act.o bias_act.cuda.o -shared -L/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/work/emar349/shared/envs/torch-gpu-stylegan/lib64 -lcudart -o bias_act_plugin.so\n/usr/bin/ld: cannot find -lcudart\ncollect2: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1540\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     stdout_fileno \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1540\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdout_fileno\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTDOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/subprocess.py:528\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 528\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    529\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   G \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fp)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_ema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m zs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m10000\u001b[39m, G\u001b[38;5;241m.\u001b[39mmapping\u001b[38;5;241m.\u001b[39mz_dim], device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 25\u001b[0m w_stds \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstd(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m<string>:151\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, z, c, truncation_psi, truncation_cutoff, update_emas)\u001b[0m\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m<string>:100\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n",
      "File \u001b[0;32m~/ml-art-test/stylegan3/./stylegan3/torch_utils/ops/bias_act.py:84\u001b[0m, in \u001b[0;36mbias_act\u001b[0;34m(x, b, dim, act, alpha, gain, clamp, impl)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impl \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _bias_act_cuda(dim\u001b[38;5;241m=\u001b[39mdim, act\u001b[38;5;241m=\u001b[39mact, alpha\u001b[38;5;241m=\u001b[39malpha, gain\u001b[38;5;241m=\u001b[39mgain, clamp\u001b[38;5;241m=\u001b[39mclamp)\u001b[38;5;241m.\u001b[39mapply(x, b)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bias_act_ref(x\u001b[38;5;241m=\u001b[39mx, b\u001b[38;5;241m=\u001b[39mb, dim\u001b[38;5;241m=\u001b[39mdim, act\u001b[38;5;241m=\u001b[39mact, alpha\u001b[38;5;241m=\u001b[39malpha, gain\u001b[38;5;241m=\u001b[39mgain, clamp\u001b[38;5;241m=\u001b[39mclamp)\n",
      "File \u001b[0;32m~/ml-art-test/stylegan3/./stylegan3/torch_utils/ops/bias_act.py:41\u001b[0m, in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _plugin\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _plugin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     _plugin \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_plugin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbias_act_plugin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbias_act.cpp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbias_act.cu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbias_act.h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--use_fast_math\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--allow-unsupported-compiler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-art-test/stylegan3/./stylegan3/torch_utils/custom_ops.py:136\u001b[0m, in \u001b[0;36mget_plugin\u001b[0;34m(module_name, sources, headers, source_dir, **build_kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Compile.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     cached_sources \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_build_dir, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(fname)) \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m sources]\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp_extension\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_build_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_build\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcpp_extension\u001b[38;5;241m.\u001b[39mload(name\u001b[38;5;241m=\u001b[39mmodule_name, verbose\u001b[38;5;241m=\u001b[39mverbose_build, sources\u001b[38;5;241m=\u001b[39msources, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuild_kwargs)\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/utils/cpp_extension.py:993\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, keep_intermediates)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(name,\n\u001b[1;32m    913\u001b[0m          sources: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    914\u001b[0m          extra_cflags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    921\u001b[0m          is_python_module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    922\u001b[0m          keep_intermediates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;124;03m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m                verbose=True)\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msources\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_build_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1200\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, keep_intermediates)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m IS_HIP_EXTENSION \u001b[38;5;129;01mand\u001b[39;00m (with_cuda \u001b[38;5;129;01mor\u001b[39;00m with_cudnn):\n\u001b[1;32m   1191\u001b[0m             hipify_python\u001b[38;5;241m.\u001b[39mhipify(\n\u001b[1;32m   1192\u001b[0m                 project_directory\u001b[38;5;241m=\u001b[39mbuild_directory,\n\u001b[1;32m   1193\u001b[0m                 output_directory\u001b[38;5;241m=\u001b[39mbuild_directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 clean_ctx\u001b[38;5;241m=\u001b[39mclean_ctx\n\u001b[1;32m   1199\u001b[0m             )\n\u001b[0;32m-> 1200\u001b[0m         \u001b[43m_write_ninja_file_and_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m            \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     baton\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1304\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuilding extension module \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m-> 1304\u001b[0m \u001b[43m_run_ninja_build\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError building extension \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/utils/cpp_extension.py:1562\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(error, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m error\u001b[38;5;241m.\u001b[39moutput:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mdecode())  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 1562\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'bias_act_plugin': [1/3] /work/emar349/shared/envs/torch-gpu-stylegan/bin/nvcc -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/TH -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/THC -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' --use_fast_math --allow-unsupported-compiler -std=c++14 -c /home/twomeylab/rtwomey/.cache/torch_extensions/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-tesla-v100-pcie-32gb/bias_act.cu -o bias_act.cuda.o \n[2/3] c++ -MMD -MF bias_act.o.d -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/TH -isystem /work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/include/THC -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include -isystem /work/emar349/shared/envs/torch-gpu-stylegan/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -c /home/twomeylab/rtwomey/.cache/torch_extensions/bias_act_plugin/3cb576a0039689487cfba59279dd6d46-tesla-v100-pcie-32gb/bias_act.cpp -o bias_act.o \n[3/3] c++ bias_act.o bias_act.cuda.o -shared -L/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/work/emar349/shared/envs/torch-gpu-stylegan/lib64 -lcudart -o bias_act_plugin.so\nFAILED: bias_act_plugin.so \nc++ bias_act.o bias_act.cuda.o -shared -L/work/emar349/shared/envs/torch-gpu-stylegan/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/work/emar349/shared/envs/torch-gpu-stylegan/lib64 -lcudart -o bias_act_plugin.so\n/usr/bin/ld: cannot find -lcudart\ncollect2: error: ld returned 1 exit status\nninja: build stopped: subcommand failed.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n",
    "\n",
    "Model = 'FFHQ' #'Wikiart' #@param [\"FFHQ\", \"MetFaces\", \"AFHQv2\", \"cosplay\", \"Wikiart\", \"Landscapes\"]\n",
    "\n",
    "model_name = {\n",
    "    \"FFHQ\": base_url + \"stylegan3-t-ffhqu-1024x1024.pkl\",\n",
    "    \"MetFaces\": base_url + \"stylegan3-r-metfacesu-1024x1024.pkl\",\n",
    "    \"AFHQv2\": base_url + \"stylegan3-t-afhqv2-512x512.pkl\",\n",
    "    \"cosplay\": \"https://l4rz.net/cosplayface-snapshot-stylegan3t-008000.pkl\",\n",
    "    \"Wikiart\": \"https://drive.google.com/u/0/open?id=18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj\",\n",
    "    \"Landscapes\": \"https://drive.google.com/u/0/open?id=14UGDDOusZ9TMb-pOrF0PAjMGVWLSAii1\"\n",
    "}\n",
    "\n",
    "# network_url = model_name[Model]\n",
    "# model_path = \"wikiart-1024-stylegan3-t-17.2Mimg.pkl\"\n",
    "# with open(fetch_model(network_url), 'rb') as fp:\n",
    "\n",
    "# model_path = \"/work/emar349/shared/sg3models/stylegan3-t-ffhqu-1024x1024.pkl\"\n",
    "model_path = \"/work/emar349/shared/sg3models/wikiart-1024-stylegan3-t-17.2Mimg.pkl\"\n",
    "\n",
    "with open(model_path, 'rb') as fp:\n",
    "  G = pickle.load(fp)['G_ema'].to(device)\n",
    "\n",
    "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
    "w_stds = G.mapping(zs, None).std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965887d5-1eff-41ec-a871-508d4b27c8eb",
   "metadata": {},
   "source": [
    "## Parameters for Text to Image Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8133a-fdac-44ce-9af8-25c0161d6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_filename = \"Red_Blossom_Cropped.jpg\" #@param {type:\"string\"}\n",
    "text = \"\" #@param {type:\"string\"}\n",
    "loss_ratio = 0.4#@param {type:\"number\"}\n",
    "steps = 800 #@param {type:\"number\"}\n",
    "limit_step = 600 #@param {type:\"number\"}\n",
    "seed = 34 # 14 default #@param {type:\"number\"}\n",
    "\n",
    "if seed == -1:\n",
    "    seed = np.random.randint(0,9e9)\n",
    "\n",
    "target = clip_model.embed_text(text)\n",
    "\n",
    "target_pil = Image.open(target_image_filename).convert('RGB')\n",
    "w, h = target_pil.size\n",
    "s = min(w, h)\n",
    "target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "target_pil = target_pil.resize((G.img_resolution, G.img_resolution), Image.LANCZOS)\n",
    "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "target_tensor = torch.tensor(target_uint8.transpose([2, 0, 1]), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d098f-2b16-4dfc-b961-444178242131",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e078c2-b525-4039-a0f2-b87cb4d93ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually do the run\n",
    "\n",
    "tf = Compose([\n",
    "  Resize(224),\n",
    "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
    "])\n",
    "\n",
    "\n",
    "def run(timestring, projection_target):\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  target_features = get_target_features(projection_target)\n",
    "\n",
    "  # Init\n",
    "  # Sample 32 inits and choose the one closest to prompt\n",
    "\n",
    "  with torch.no_grad():\n",
    "    qs = []\n",
    "    losses = []\n",
    "    for _ in range(8):\n",
    "      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
    "      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
    "      loss = get_perceptual_loss(images, target_features)\n",
    "      i = torch.argmin(loss)\n",
    "      qs.append(q[i])\n",
    "      losses.append(loss)\n",
    "    qs = torch.stack(qs)\n",
    "    losses = torch.stack(losses)\n",
    "    i = torch.argmin(losses)\n",
    "    q = qs[i].unsqueeze(0).requires_grad_()\n",
    "\n",
    "  # Sampling loop\n",
    "  q_ema = q\n",
    "  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n",
    "  loop = tqdm(range(steps))\n",
    "  for i in loop:\n",
    "    opt.zero_grad()\n",
    "    w = q * w_stds\n",
    "    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
    "    embed = embed_image(image.add(1).div(2))\n",
    "    step_ratio = i / limit_step\n",
    "    perceptual_loss = get_perceptual_loss(image, target_features)\n",
    "    modulated_perceptual_loss = (\n",
    "        max(loss_ratio, 1 - step_ratio)\n",
    "        * get_perceptual_loss(image, target_features)\n",
    "    )\n",
    "    clip_loss = spherical_dist_loss(embed, target).mean()\n",
    "    modulated_clip_loss = (\n",
    "        min(1 - loss_ratio, step_ratio)\n",
    "        * (step_ratio) * spherical_dist_loss(embed, target).mean()\n",
    "    )\n",
    "    loss = modulated_perceptual_loss + modulated_clip_loss\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
    "\n",
    "    q_ema = q_ema * 0.9 + q * 0.1\n",
    "    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n",
    "\n",
    "    if i % 10 == 0:\n",
    "      display(TF.to_pil_image(tf(image)[0]))\n",
    "      print(f\"image {i}/{steps} | projector loss: {perceptual_loss} | clip loss: {clip_loss} | modulated loss: {loss}\")\n",
    "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
    "    os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
    "    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n",
    "\n",
    "try:\n",
    "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
    "  run(timestring, target_tensor)\n",
    "except KeyboardInterrupt:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a933f9-a0c1-431f-b509-0389572e5207",
   "metadata": {},
   "source": [
    "## Save files\n",
    "\n",
    "download the tar file created by this step manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b1fd1-b40d-4ccc-927b-2bf61b539eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown #**Save images** 📷\n",
    "#@markdown A `.tar` file will be saved inside *samples* and automatically downloaded, unless you previously ran the Google Drive cell,\n",
    "#@markdown in which case it'll be saved inside your previously created drive *samples* folder.\n",
    "\n",
    "archive_name = \"optional\"#@param {type:\"string\"}\n",
    "\n",
    "archive_name = slugify(archive_name)\n",
    "\n",
    "if archive_name != \"optional\":\n",
    "  fname = archive_name\n",
    "  # os.rename(f'samples/{timestring}', f'samples/{fname}')\n",
    "else:\n",
    "  fname = timestring\n",
    "# Save images as a tar archive\n",
    "!tar cf samples/{fname}.tar samples/{timestring}\n",
    "\n",
    "## requires the google files extension\n",
    "# if os.path.isdir('drive/MyDrive/samples'):\n",
    "#   shutil.copyfile(f'samples/{fname}.tar', f'drive/MyDrive/samples/{fname}.tar')\n",
    "# else:\n",
    "#   files.download(f'samples/{fname}.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510aed0a-87a1-4b51-b093-a246a52b97bd",
   "metadata": {},
   "source": [
    "## Make a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d18136-398a-4718-8bef-1d0cac544afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown You can edit frame rate and stuff by double-clicking this tab.\n",
    "\n",
    "frames = os.listdir(f\"samples/{timestring}\")\n",
    "frames = len(list(filter(lambda filename: filename.endswith(\".jpg\"), frames))) #Get number of jpg generated\n",
    "\n",
    "init_frame = 1 #This is the frame where the video will start\n",
    "last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
    "\n",
    "min_fps = 10\n",
    "max_fps = 60\n",
    "\n",
    "total_frames = last_frame-init_frame\n",
    "\n",
    "#Desired video time in seconds\n",
    "video_length = 14 #@param {type:\"number\"}\n",
    "#Video filename\n",
    "video_name = \"\" #@param {type:\"string\"}\n",
    "video_name = target_image_filename\n",
    "video_name = slugify(video_name)\n",
    "\n",
    "# frames = []\n",
    "# tqdm.write('Generating video...')\n",
    "# for i in range(init_frame,last_frame): #\n",
    "#     filename = f\"samples/{timestring}/{i:04}.jpg\"\n",
    "#     frames.append(Image.open(filename))\n",
    "\n",
    "fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
    "\n",
    "!ffmpeg -r {fps} -i samples/{timestring}/%04d.jpg -c:v libx264 -vf fps={fps} -pix_fmt yuv420p samples/{video_name}.mp4 -frames:v {total_frames}\n",
    "\n",
    "# from subprocess import Popen, PIPE\n",
    "# p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', f'samples/{video_name}.mp4'], stdin=PIPE)\n",
    "# for im in tqdm(frames):\n",
    "#     im.save(p.stdin, 'PNG')\n",
    "# p.stdin.close()\n",
    "\n",
    "# print(\"The video is now being compressed, wait...\")\n",
    "# p.wait()\n",
    "# print(\"The video is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f6df1-5d4b-42c2-a94e-724ca73fc97a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b5670-f7dc-4d79-b2b4-8b79cb69d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-gpu-clip)",
   "language": "python",
   "name": "torch-gpu-clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
