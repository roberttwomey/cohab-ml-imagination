{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwTP4MYk0bYn"
   },
   "source": [
    "## Generate Interpolations from Stored Vectors\n",
    "\n",
    "Generating interpolation videos from POM2021 performance. \n",
    "\n",
    "Based on [j.mp/wanderclip](https://j.mp/wanderclip) by Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com)\n",
    "\n",
    "Modified to run on HCC OOD/nautilus.optiputer.net/z8 by robert.twomey@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Run Once\n",
    "\n",
    "Onetime setup (jupyter environment). Installs necessary python packages. (you may need to restart the kernel after this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ipython-autotime in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (0.3.1)\n",
      "Requirement already satisfied: ipython in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython-autotime) (7.30.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (59.4.0)\n",
      "Requirement already satisfied: backcall in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: pygments in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (0.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (5.1.1)\n",
      "Requirement already satisfied: decorator in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (5.1.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (3.0.24)\n",
      "Requirement already satisfied: jedi>=0.16 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ipython->ipython-autotime) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.1+cu101 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (1.7.1+cu101)\n",
      "Requirement already satisfied: torchvision==0.8.2+cu101 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (0.8.2+cu101)\n",
      "Requirement already satisfied: ftfy in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (6.0.3)\n",
      "Requirement already satisfied: regex in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (2021.11.10)\n",
      "Requirement already satisfied: numpy in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from torch==1.7.1+cu101) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from torch==1.7.1+cu101) (4.0.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from torchvision==0.8.2+cu101) (8.3.1)\n",
      "Requirement already satisfied: wcwidth in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from ftfy) (0.2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytorch-pretrained-biggan in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (0.1.1)\n",
      "Requirement already satisfied: numpy in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from pytorch-pretrained-biggan) (1.21.4)\n",
      "Requirement already satisfied: tqdm in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from pytorch-pretrained-biggan) (4.62.3)\n",
      "Requirement already satisfied: boto3 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (from pytorch-pretrained-biggan) (1.18.57)\n",
      "Requirement already satisfied: requests in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from pytorch-pretrained-biggan) (2.26.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (from pytorch-pretrained-biggan) (1.7.1+cu101)\n",
      "Requirement already satisfied: typing-extensions in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (4.0.1)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.57 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (from boto3->pytorch-pretrained-biggan) (1.21.57)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (from boto3->pytorch-pretrained-biggan) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (from boto3->pytorch-pretrained-biggan) (0.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from requests->pytorch-pretrained-biggan) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from requests->pytorch-pretrained-biggan) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from requests->pytorch-pretrained-biggan) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from requests->pytorch-pretrained-biggan) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from botocore<1.22.0,>=1.21.57->boto3->pytorch-pretrained-biggan) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.57->boto3->pytorch-pretrained-biggan) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (3.6.3)\n",
      "Requirement already satisfied: tqdm in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /util/opt/anaconda/deployed-conda-envs/packages/jupyterlab/envs/jupyterlab-3.0.16/lib/python3.8/site-packages (from nltk) (8.0.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: cma in /home/twomeylab/rtwomey/.local/lib/python3.8/site-packages (3.1.0)\n",
      "time: 5.78 s (started: 2021-12-23 09:58:28 -06:00)\n"
     ]
    }
   ],
   "source": [
    "# !pip install perlin-noise\n",
    "!pip install ipython-autotime\n",
    "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
    "!pip install pytorch-pretrained-biggan\n",
    "!pip install nltk\n",
    "!pip install cma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the timer in case we want it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 748 µs (started: 2021-12-23 09:58:52 -06:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/twomeylab/rtwomey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded bigGAN\n",
      "time: 3.52 s (started: 2021-12-23 09:58:55 -06:00)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, clear_output\n",
    "from PIL import Image\n",
    "from IPython.display import Image as JupImage\n",
    "import numpy as np\n",
    "import nltk\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# from biggan\n",
    "import torch\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
    "                                       save_as_images, convert_to_images) #, display_in_terminal)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# do we need wordnet?\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# load biggan\n",
    "model = BigGAN.from_pretrained('biggan-deep-512')\n",
    "print(\"loaded bigGAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More imports for interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 524 µs (started: 2021-12-23 09:58:58 -06:00)\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from numpy import arccos\n",
    "from numpy import clip\n",
    "from numpy import dot\n",
    "from numpy import sin\n",
    "from numpy import linspace\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 709 µs (started: 2021-12-23 09:59:01 -06:00)\n"
     ]
    }
   ],
   "source": [
    "# from\n",
    "# https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/4\n",
    "\n",
    "# spherical linear interpolation (slerp)\n",
    "def slerp(val, low, high):\n",
    "    omega = arccos(clip(dot(low/norm(low), high/norm(high)), -1, 1))\n",
    "    so = sin(omega)\n",
    "    if so == 0:\n",
    "        # L'Hopital's rule/LERP\n",
    "        return (1.0-val) * low + val * high\n",
    "    return sin((1.0-val)*omega) / so * low + sin(val*omega) / so * high\n",
    " \n",
    "# uniform interpolation between two points in latent space\n",
    "def interpolate_points(p1, p2, n_steps=10):\n",
    "    # interpolate ratios between the points\n",
    "    ratios = np.linspace(0, 1, num=n_steps)\n",
    "    # linear interpolate vectors\n",
    "    vectors = list()\n",
    "    for ratio in ratios:\n",
    "        v = slerp(ratio, p1, p2)\n",
    "        vectors.append(v)\n",
    "    return np.asarray(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File output settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 849 µs (started: 2021-12-23 09:59:02 -06:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# file paths\n",
    "work = os.getcwd() # get the current path\n",
    "\n",
    "# No need to seed the interpolation for generation from stored vectors\n",
    "# np.random.RandomState(1)\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# one random variable\n",
    "truncation = 1.0\n",
    "\n",
    "# the file directories\n",
    "workbase = work\n",
    "\n",
    "# results of text to image translation\n",
    "resultspath = os.path.join(work, \"results/\")\n",
    "\n",
    "# intermediate frames working directory\n",
    "interpbase = os.path.join(work, \"interpolation/\")\n",
    "\n",
    "# video directory\n",
    "videopath = os.path.join(work, \"videos/\")\n",
    "\n",
    "# video and srt output files\n",
    "# filebase = '{0}'\n",
    "# moviefilename = filebase+'.mp4'\n",
    "# srtfilename = filebase+'.srt'\n",
    "\n",
    "# the interpolation\n",
    "num_steps = 90#300\n",
    "len_hold = 30\n",
    "\n",
    "# frame rate for the the movie\n",
    "fps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 289 ms (started: 2021-12-23 09:59:02 -06:00)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p $interpbase\n",
    "!mkdir -p $videopath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Interpolation Between Two Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 598 µs (started: 2021-12-23 09:59:06 -06:00)\n"
     ]
    }
   ],
   "source": [
    "def frames_to_TC (frames):\n",
    "    h = int(frames / 86400) \n",
    "    m = int(frames / (60*fps)) % 60 \n",
    "    s = int((frames % (60*fps))/fps) \n",
    "    f = float(frames % (60*fps) % fps)/float(fps)\n",
    "    return ( \"%02d:%02d:%02d,%04d\" % ( h, m, s, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 421 µs (started: 2021-12-23 09:59:10 -06:00)\n"
     ]
    }
   ],
   "source": [
    "def make_safe_filename(s):\n",
    "    def safe_char(c):\n",
    "        if c.isalnum():\n",
    "            return c\n",
    "        else:\n",
    "            return \"_\"\n",
    "    return \"\".join(safe_char(c) for c in s).rstrip(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.08 ms (started: 2021-12-23 09:59:10 -06:00)\n"
     ]
    }
   ],
   "source": [
    "def generate_video_transition(class_filenames, prompts, interpbase):\n",
    "    \"\"\"includes the result with the class_files\"\"\"\n",
    "    \n",
    "    print(\"==== retrieve vectors ====\")    \n",
    "    safe_prompt = make_safe_filename(prompts[0])\n",
    "    noise_filenames = [fname.replace(\"class\", \"noise\") for fname in class_filenames]\n",
    "\n",
    "    #print(class_filenames)\n",
    "    #print(noise_filenames)\n",
    "\n",
    "    class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "    noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "\n",
    "    # file outputs names and paths\n",
    "    moviefilename = safe_prompt+'_{0}.mp4'.format(fps)\n",
    "    srtfilename = safe_prompt+'_{0}.srt'.format(fps)\n",
    "    interppath = os.path.join(interpbase, safe_prompt)\n",
    "\n",
    "    !mkdir -p $interppath\n",
    "    \n",
    "    print(\"==== generate interpolations ====\")\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(class_inputs)):\n",
    "\n",
    "        # generate interpolations\n",
    "        noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "        classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "        # generate images in batches\n",
    "#         batch_size = 10\n",
    "        batch_size = 40 #RTX8000\n",
    "    \n",
    "        for j in range(0, num_steps, batch_size):\n",
    "            clear_output()\n",
    "            print(i, j, count)\n",
    "            noise_vector = noises[j:j+batch_size]\n",
    "            class_vector = classes[j:j+batch_size]\n",
    "\n",
    "            # convert to tensors\n",
    "            noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "            class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "            # put everything on cuda (GPU)\n",
    "            noise_vector = noise_vector.to('cuda')\n",
    "            noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "            class_vector = class_vector.to('cuda')\n",
    "            class_vector = class_vector.softmax(dim=-1)\n",
    "            model.to('cuda')\n",
    "\n",
    "            # generate images\n",
    "            with torch.no_grad():\n",
    "                output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "            # If you have a GPU put back on CPU\n",
    "            output = output.to('cpu')\n",
    "\n",
    "            imgs = convert_to_images(output)\n",
    "\n",
    "            # repeat first image for len_hold\n",
    "            if j == 0:\n",
    "                for k in range(len_hold):\n",
    "                    imgs[0].save(interppath+\"/output_%05d.png\" % count)\n",
    "                    count = count + 1\n",
    "\n",
    "            for img in imgs: \n",
    "                img.save(interppath+\"/output_%05d.png\" % count)\n",
    "                count = count + 1\n",
    "    \n",
    "    \n",
    "    print(\"==== generating movie ====\")\n",
    "    \n",
    "    # generate mp4\n",
    "    outpath = os.path.join(videopath, moviefilename)\n",
    "    with open('list.txt','w') as f:\n",
    "      for i in range(count):\n",
    "        f.write('file %s/output_%05d.png\\n'%(interppath, i))\n",
    "    cmd = \"ffmpeg -r {0} -i {1}/output_%05d.png -c:v libx265 -pix_fmt yuv420p -crf 0 -r {0} {2} -y\"\n",
    "    \n",
    "    print(\"video creation cmd:\\n\", cmd.format(fps, interppath, outpath))\n",
    "#     os.system(cmd.format(fps, interppath, outpath))\n",
    "    \n",
    "    print(\"==== generating subtitles ==== \")\n",
    "    srtoutpath = os.path.join(videopath, srtfilename)\n",
    "    with open(srtoutpath,'w') as f:\n",
    "\n",
    "        frames = 0\n",
    "        for i in range(len(class_inputs)):\n",
    "            f.write(str(i+1)+\"\\n\")\n",
    "            f.write(frames_to_TC(frames)+\" --> \")\n",
    "            frames+=len_hold\n",
    "            f.write(frames_to_TC(frames)+\"\\n\")\n",
    "            f.write(prompts[i]+\"\\n\\n\")\n",
    "            frames+=num_steps\n",
    "    print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 5.00 GiB (GPU 0; 15.78 GiB total capacity; 11.69 GiB already allocated; 2.42 GiB free; 12.18 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_661553/958208913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclass_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresultspath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlatent\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlatentfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgenerate_video_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpbase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_661553/1823233209.py\u001b[0m in \u001b[0;36mgenerate_video_transition\u001b[0;34m(class_filenames, prompts, interpbase)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# generate images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# If you have a GPU put back on CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_pretrained_biggan/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, class_label, truncation)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mcond_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_pretrained_biggan/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, cond_vector, truncation)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_pretrained_biggan/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, cond_vector, truncation)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 5.00 GiB (GPU 0; 15.78 GiB total capacity; 11.69 GiB already allocated; 2.42 GiB free; 12.18 GiB reserved in total by PyTorch)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.87 s (started: 2021-12-23 09:59:11 -06:00)\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"a drawing of an elegant machine\",\n",
    "    \"a machine that learns to make images\"\n",
    "]\n",
    "\n",
    "latentfiles = [\n",
    "    \"a_drawing_of_an_elegant_machine_class.txt\",\n",
    "    \"a_machine_that_learns_to_make_images_class.txt\"\n",
    "]\n",
    "\n",
    "class_filenames = [resultspath+latent for latent in latentfiles]\n",
    "    \n",
    "generate_video_transition(class_filenames, prompts, interpbase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Video Interpolating Between Multiple Version of one Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper for generating time codes for subtitles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 511 µs (started: 2021-12-21 15:29:11 -06:00)\n"
     ]
    }
   ],
   "source": [
    "def frames_to_TC (frames):\n",
    "    h = int(frames / 86400) \n",
    "    m = int(frames / (60*fps)) % 60 \n",
    "    s = int((frames % (60*fps))/fps) \n",
    "    f = float(frames % (60*fps) % fps)/float(fps)\n",
    "    return ( \"%02d:%02d:%02d,%04d\" % ( h, m, s, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 341 µs (started: 2021-12-21 15:29:12 -06:00)\n"
     ]
    }
   ],
   "source": [
    "def make_safe_filename(s):\n",
    "    def safe_char(c):\n",
    "        if c.isalnum():\n",
    "            return c\n",
    "        else:\n",
    "            return \"_\"\n",
    "    return \"\".join(safe_char(c) for c in s).rstrip(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_hold(output_folder, class_filenames, result_filename, prompt, interpbase, filebase):\n",
    "    \"\"\"includes the result with the class_files\"\"\"\n",
    "    \n",
    "    print(\"==== retrieve vectors ====\")\n",
    "    \n",
    "    safe_prompt = make_safe_filename(prompt)\n",
    "\n",
    "    prompts = [ prompt*(len(class_filenames)+1)]\n",
    "\n",
    "    class_filenames = [output_folder+file for file in class_filenames]\n",
    "    class_filenames += [result_filename.format(safe_prompt)]\n",
    "\n",
    "    noise_filenames = [fname.replace(\"class\", \"noise\") for fname in class_filenames]\n",
    "\n",
    "    print(class_filenames)\n",
    "    print(noise_filenames)\n",
    "\n",
    "    class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "    noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "\n",
    "\n",
    "    interpbase = interpbase.format(safe_prompt)\n",
    "    filebase = filebase.format(safe_prompt)\n",
    "    moviefilename = filebase+'.mp4'\n",
    "    srtfilename = filebase+'.srt'\n",
    "\n",
    "    !mkdir -p $interpbase\n",
    "    \n",
    "    print(\"==== generate interpolations ====\")\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    num_inputs = len(class_inputs)\n",
    "    for i in range(len(class_inputs)):\n",
    "\n",
    "        # generate interpolations\n",
    "        noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "        classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "        # generate images in batches\n",
    "#         batch_size = 10 #2080\n",
    "        batch_size = 60 #RTX8000\n",
    "        for j in range(0, num_steps, batch_size):\n",
    "            clear_output()\n",
    "            print(\"{0}/{1} clases\\t{2}/{3} steps\\t{4} frames\".format(i, num_inputs, j, num_steps, count))\n",
    "            noise_vector = noises[j:j+batch_size]\n",
    "            class_vector = classes[j:j+batch_size]\n",
    "\n",
    "            # convert to tensors\n",
    "            noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "            class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "            # put everything on cuda (GPU)\n",
    "            noise_vector = noise_vector.to('cuda')\n",
    "            noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "            class_vector = class_vector.to('cuda')\n",
    "            class_vector = class_vector.softmax(dim=-1)\n",
    "            model.to('cuda')\n",
    "\n",
    "            # generate images\n",
    "            with torch.no_grad():\n",
    "                output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "            # If you have a GPU put back on CPU\n",
    "            output = output.to('cpu')\n",
    "\n",
    "            imgs = convert_to_images(output)\n",
    "\n",
    "            # repeat first image\n",
    "\n",
    "            if j == 0:\n",
    "                for k in range(len_hold):\n",
    "                    imgs[0].save(interpbase+\"/output_%05d.png\" % count)\n",
    "                    count = count + 1\n",
    "\n",
    "            for img in imgs: \n",
    "                img.save(interpbase+\"/output_%05d.png\" % count)\n",
    "                count = count + 1\n",
    "    \n",
    "    \n",
    "    print(\"==== generating movie ====\")\n",
    "    \n",
    "    # generate mp4\n",
    "    out = moviefilename%fps\n",
    "    with open('list.txt','w') as f:\n",
    "      for i in range(count):\n",
    "        f.write('file %s/output_%05d.png\\n'%(interpbase, i))\n",
    "    cmd = \"ffmpeg -r {0} -i {1}/output_%05d.png -c:v libx265 -pix_fmt yuv420p -crf 0 -r {0} {2} -y\"\n",
    "\n",
    "    os.system(cmd.format(fps, interpbase, out))\n",
    "    print(cmd.format(fps, interpbase, out))\n",
    "    \n",
    "    print(\"==== generating subtitles ==== \")\n",
    "    \n",
    "    with open(srtfilename%fps,'w') as f:\n",
    "\n",
    "        frames = 0\n",
    "        for i in range(len(class_inputs)):\n",
    "            f.write(str(i+1)+\"\\n\")\n",
    "            f.write(frames_to_TC(frames)+\" --> \")\n",
    "            frames+=len_hold\n",
    "            f.write(frames_to_TC(frames)+\"\\n\")\n",
    "            f.write(prompt+\"\\n\\n\")\n",
    "            frames+=num_steps\n",
    "    print(\"subtitles: \"+srtfilename%fps)\n",
    "    print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resultsbase = \"/home/jovyan/work/results/results_pom_seed_128/\"\n",
    "\n",
    "# # video and srt output files\n",
    "# filebase = 'pom_128_%s'\n",
    "# # filebase = 'pom_10_%s'\n",
    "# moviefilename = filebase+'.mp4'\n",
    "# srtfilename = filebase+'.srt'\n",
    "\n",
    "# prompts = [\n",
    "# #     \"the sublime experience of an iceberg\",\n",
    "# #     \"ice is a vastness of possibilities\",\n",
    "# #     \"a vastness of spatial dimensions\",\n",
    "# #     \"a space of unlimited possibilities that the network must explore\",\n",
    "# #     \"a network\",\n",
    "# #     \"a network of vast spatial dimensions\",\n",
    "# #     \"eighteenth century painting of humans encountering nature\",\n",
    "# #     \"a person encountering nature\",\n",
    "# #     \"an elegant machine that learns to generate artificial images\",\n",
    "# #     \"a drawing of an elegant machine\",\n",
    "# #     \"a machine that learns to make images\",\n",
    "#     \"a drawing of a machine that learns to make images\",\n",
    "# ]\n",
    "\n",
    "# for prompt in prompts:\n",
    "    \n",
    "#     # prompt = \"a silicon mold of a friends hands\"\n",
    "\n",
    "#     safe_prompt = make_safe_filename(prompt)\n",
    "\n",
    "#     class_filenames = [ \n",
    "#         \"class_00009.txt\", \n",
    "#     #     \"class_00008.txt\", \n",
    "#         \"class_00007.txt\"\n",
    "#     ]\n",
    "\n",
    "#     result_filename = resultsbase+\"{0}_class.txt\"\n",
    "\n",
    "#     # stored output from synthesis\n",
    "#     # safe_filename = make_safe_filename(prompt)\n",
    "\n",
    "#     stored_output_folder = \"/home/jovyan/work/process/stored_outputs_pom_seed_128/\"+safe_prompt+\"/\"\n",
    "\n",
    "#     # stored_output_folder = \"/home/jovyan/work/process/stored_outputs_pom_seed_128/\"+\\\n",
    "#     #     \"a_mushrooms_above_ground_growth_is_the_result_of_a_set_of_instructions_20210914_214118/\"\n",
    "\n",
    "#     interpbase = '/home/jovyan/work/interpolation/pom_interp_128_hold_{0}'\n",
    "#     filebase = '/home/jovyan/work/interpolation/pom_128_%s_hold_{0}'\n",
    "\n",
    "#     generate_video_hold(stored_output_folder, class_filenames, result_filename, prompt, interpbase, filebase)\n",
    "#     print('scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_{0}*\" .'.format(safe_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workaround to download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"the sublime experience of an iceberg\",\n",
    "#     \"ice is a vastness of possibilities\",\n",
    "#     \"a vastness of spatial dimensions\",\n",
    "#     \"a space of unlimited possibilities that the network must explore\",\n",
    "#     \"a network\",\n",
    "#     \"a network of vast spatial dimensions\"\n",
    "#     \"eighteenth century painting of humans encountering nature\",\n",
    "#     \"a person encountering nature\",\n",
    "#     \"an elegant machine that learns to generate artificial images\",\n",
    "#     \"a drawing of an elegant machine\",\n",
    "#     \"a machine that learns to make images\",\n",
    "#     \"a drawing of a machine that learns to make images\",\n",
    "# ]\n",
    "# for prompt in prompts:\n",
    "    \n",
    "#     # prompt = \"a silicon mold of a friends hands\"\n",
    "\n",
    "#     safe_prompt = make_safe_filename(prompt)\n",
    "#     print('scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_{0}*\" .'.format(safe_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on SIREN+CLIP Colabs by: [@advadnoun](https://twitter.com/advadnoun), [@norod78](https://twitter.com/norod78)\n",
    "\n",
    "Other CLIP notebooks: [OpenAI tutorial](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb), [SIREN by @advadnoun](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP), [SIREN by @norod78](https://colab.research.google.com/drive/1K1vfpTEvAmxW2rnhAaALRVyis8EiLOnD), [BigGAN by @advadnoun](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR), [BigGAN by @eyaler](j.mp/bigclip), [BigGAN by @tg_bomze](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Image_v2.ipynb), [BigGAN using big-sleep library by @lucidrains](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb), [BigGAN story hallucinator by @bonkerfield](https://colab.research.google.com/drive/1jF8pyZ7uaNYbk9ZiVdxTOajkp8kbmkLK), [StyleGAN2-ADA Anime by @nagolinc](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb) [v2](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/CLIP_%2B_TADNE_(pytorch)_v2.ipynb)\n",
    "\n",
    "Using the works:\n",
    "\n",
    "https://github.com/openai/CLIP\n",
    "\n",
    "https://tfhub.dev/deepmind/biggan-deep-512\n",
    "\n",
    "https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
    "\n",
    "http://www.aiartonline.com/design-2019/eyal-gruss (WanderGAN)\n",
    "\n",
    "For a curated list of more online generative tools see: [j.mp/generativetools](https://j.mp/generativetools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POM Prompts/Themes\n",
    "\n",
    "We need to generate interpolation videos between each of the prompts in the following groups of themes/prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro section\n",
    "\n",
    "hold videos ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/r/personal/rtwomey2_unl_edu/Documents/POM_AI/pom_videos_hold.zip?csf=1&web=1&e=PF8MQN)):\n",
    "```\n",
    "pom_128_30_hold_the_sublime_experience_of_an_iceberg.mp4\n",
    "pom_128_30_hold_ice_is_a_vastness_of_possibilities.mp4\n",
    "pom_128_30_hold_a_vastness_of_spatial_dimensions.mp4\n",
    "pom_128_30_hold_a_space_of_unlimited_possibilities_that_the_network_must_explore.mp4\n",
    "pom_128_30_hold_a_network.mp4\n",
    "pom_128_30_hold_a_network_of_vast_spatial_dimensions.mp4\n",
    "pom_128_30_hold_a_person_encountering_nature.mp4\n",
    "pom_128_30_hold_eighteenth_century_painting_of_humans_encountering_nature.mp4\n",
    "pom_128_30_hold_an_elegant_machine_that_learns_to_generate_artificial_images.mp4\n",
    "pom_128_30_hold_a_drawing_of_an_elegant_machine.mp4\n",
    "pom_128_30_hold_a_machine_that_learns_to_make_images.mp4\n",
    "pom_128_30_hold_a_drawing_of_a_machine_that_learns_to_make_images.mp4\n",
    "```\n",
    "\n",
    "interpolation videos to be generated:\n",
    "- one video transitioning between each of the phrases in this list (1->2, 2->3, 3->4, n->1)\n",
    "- videos of transitions between all pairwise combinations of the list\n",
    "\n",
    "latent vectors for each phrase are in `results_pom_seed_128.tar.gz` ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/g/personal/rtwomey2_unl_edu/EVB51YvooN5Fu_0l7jQJhbkB_lx97kcyQ5oBL4Md6Dvvug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # intro\n",
    "#     \"the sublime experience of an iceberg\",\n",
    "#     \"ice is a vastness of possibilities\",\n",
    "#     \"the vastness of possibilities of water is not a spatial vastness\",\n",
    "#     \"a vastness of spatial dimensions\",\n",
    "#     \"a space of unlimited possibilities that the network must explore\"\n",
    "#     \"a network\",\n",
    "#     \"a network of vast spatial dimensions\",\n",
    "#     \"a person encountering nature\",\n",
    "#     \"eighteenth century painting of humans encountering nature\",\n",
    "#     \"an elegant machine that learns to generate artificial images\",\n",
    "#     \"a drawing of an elegant machine\",\n",
    "#     \"a machine that learns to make images\",\n",
    "#     \"a drawing of a machine that learns to make images\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme 1 - On Plants, as Potential\n",
    "\n",
    "hold videos ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/r/personal/rtwomey2_unl_edu/Documents/POM_AI/pom_videos_hold.zip?csf=1&web=1&e=PF8MQN)):\n",
    "```\n",
    "pom_128_30_hold_a_seed__a_plant_s_basic_unit__remains_ever_in_potentia.mp4\n",
    "pom_128_30_hold_they_make_us_have_arms_and_not_wings.mp4\n",
    "pom_128_30_hold_a_mushroom_is_the_aerial_manifestation_of_a_larger_infestation.mp4\n",
    "pom_128_30_hold_growing_mushrooms_in_the_fire_stricken_forests_of_LA.mp4\n",
    "pom_128_30_hold_growing_mushrooms_in_the_forests_is_a_way_to_uncover_their_potential.mp4\n",
    "pom_128_30_hold_a_mushrooms_above_ground_growth_is_the_result_of_a_set_of_instructions.mp4\n",
    "```\n",
    "\n",
    "interpolation videos to be generated:\n",
    "- one video transitioning between each of the phrases in this list (1->2, 2->3, 3->4, n->1)\n",
    "- videos of transitions between all pairwise combinations of the list\n",
    "\n",
    "latent vectors for each phrase are in `results_pom_seed_128.tar.gz` ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/g/personal/rtwomey2_unl_edu/EVB51YvooN5Fu_0l7jQJhbkB_lx97kcyQ5oBL4Md6Dvvug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # theme 1\n",
    "#     \"a mushroom is the aerial manifestation of a larger infestation\",\n",
    "#     \"a mushrooms above ground growth is the result of a set of instructions\",\n",
    "#     \"growing mushrooms in the fire-stricken forests of LA\",\n",
    "#     \"growing mushrooms in the forests is a way to uncover their potential\"    \n",
    "#     \"a seed, a plant's basic unit, remains ever in potentia\",\n",
    "#     \"they make us have arms and not wings\", \n",
    "#     \"the fire-stricken forests of LA\",\n",
    "#     \"the fruiting body of a larger organism\"\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme 2 - On Our Relationships with Machines\n",
    "\n",
    "hold videos ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/r/personal/rtwomey2_unl_edu/Documents/POM_AI/pom_videos_hold.zip?csf=1&web=1&e=PF8MQN)):\n",
    "```\n",
    "pom_128_30_hold_a_family_of_robotic_seals.mp4\n",
    "pom_128_30_hold_nursing_homes_in_japan.mp4\n",
    "pom_128_30_hold_to_help_the_elderly_cope.mp4\n",
    "pom_128_30_hold_the_after_effects_of_the_Tsunami.mp4\n",
    "pom_128_30_hold_the_children_laugh_and_smile.mp4\n",
    "pom_128_30_hold_their_homes_washed_away.mp4\n",
    "pom_128_30_hold_watching_the_news_on_television.mp4\n",
    "pom_128_30_hold_the_glow_of_a_television_screen.mp4\n",
    "```\n",
    "\n",
    "interpolation videos to be generated:\n",
    "- one video transitioning between each of the phrases in this list (1->2, 2->3, 3->4, n->1)\n",
    "- videos of transitions between all pairwise combinations of the list\n",
    "\n",
    "latent vectors for each phrase are in `results_pom_seed_128.tar.gz` ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/g/personal/rtwomey2_unl_edu/EVB51YvooN5Fu_0l7jQJhbkB_lx97kcyQ5oBL4Md6Dvvug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # theme2\n",
    "#     \"a family of robotic seals\",\n",
    "#     \"nursing homes in japan\",\n",
    "#     \"to help the elderly cope\", \n",
    "#     \"the after effects of the Tsunami\",\n",
    "#     \"the children laugh and smile\",\n",
    "#     \"their homes washed away\",\n",
    "#     \"watching the news on television\",\n",
    "#     \"the glow of a television screen\",\n",
    "#     \"spoke through a microphone\",\n",
    "#     \"the robotic seals danced\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme 3 - On Care\n",
    "\n",
    "hold videos ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/r/personal/rtwomey2_unl_edu/Documents/POM_AI/pom_videos_hold.zip?csf=1&web=1&e=PF8MQN)):\n",
    "```\n",
    "pom_128_30_hold_an_xray_of_the_first_silicon_chip.mp4            \n",
    "pom_128_30_hold_the_dover_demonstration_chip.mp4\n",
    "pom_128_30_hold_a_picture_of_his_arthritic_hands.mp4\n",
    "pom_128_30_hold_a_piece_of_wire_to_connect_two_silicon_squares.mp4\n",
    "pom_128_30_hold_smaller_and_smaller_computer_chips.mp4\n",
    "pom_128_30_hold_the_kilby_diode.mp4\n",
    "pom_128_30_hold_a_silicon_mold_of_a_friends_hands.mp4\n",
    "pom_128_30_hold_in_these_containers__we_care_for_machines.mp4\n",
    "```\n",
    "\n",
    "interpolation videos to be generated:\n",
    "- one video transitioning between each of the phrases in this list (1->2, 2->3, 3->4, n->1)\n",
    "- videos of transitions between all pairwise combinations of the list\n",
    "\n",
    "latent vectors for each phrase are in `results_pom_seed_128.tar.gz` ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/g/personal/rtwomey2_unl_edu/EVB51YvooN5Fu_0l7jQJhbkB_lx97kcyQ5oBL4Md6Dvvug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # theme 3\n",
    "#     \"an xray of the first silicon chip\",\n",
    "#     \"the dover demonstration chip\",\n",
    "#     \"a picture of his arthritic hands\",\n",
    "#     \"a piece of wire to connect two silicon squares\",\n",
    "#     \"smaller and smaller computer chips\",\n",
    "#     \"the kilby diode\",\n",
    "#     \"a silicon mold of a friends hands\"\n",
    "#     \"in these containers, we care for machines\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme 4 - On Extraction\n",
    "\n",
    "hold videos ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/r/personal/rtwomey2_unl_edu/Documents/POM_AI/pom_videos_hold.zip?csf=1&web=1&e=PF8MQN)):\n",
    "```\n",
    "pom_128_30_hold_the_first_computing_devices_were_powered_by_steam_engines.mp4\n",
    "pom_128_30_hold_fueled_with_the_burning_of_fossilized_coal.mp4\n",
    "pom_128_30_hold_the_bones_of_sparse_data_structures.mp4\n",
    "pom_128_30_hold_the_slow_demise_of_our_geological_resources.mp4\n",
    "pom_128_30_hold_extracting_lithium.mp4\n",
    "pom_128_30_hold_battery_technology.mp4\n",
    "pom_128_30_hold_computing_devices_can_be_charged_without_being_plugged_in.mp4\n",
    "pom_128_30_hold_key_contributor_to_climate_change.mp4\n",
    "pom_128_30_hold_we_are_powered_by_imagination.mp4\n",
    "pom_128_30_hold_the_machines_we_use_to_think.mp4\n",
    "pom_128_30_hold_the_machines_that_are_destroying_the_planet.mp4\n",
    "```\n",
    "\n",
    "interpolation videos to be generated:\n",
    "- one video transitioning between each of the phrases in this list (1->2, 2->3, 3->4, n->1)\n",
    "- videos of transitions between all pairwise combinations of the list\n",
    "\n",
    "latent vectors for each phrase are in `results_pom_seed_128.tar.gz` ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/g/personal/rtwomey2_unl_edu/EVB51YvooN5Fu_0l7jQJhbkB_lx97kcyQ5oBL4Md6Dvvug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     #theme 4\n",
    "#     \"the first computing devices were powered by steam engines\",\n",
    "#     \"fueled with the burning of fossilized coal\",\n",
    "#     \"the bones of sparse data structures\",\n",
    "#     \"the slow demise of our geological resources\",\n",
    "#     \"extracting lithium\",\n",
    "#     \"battery technology\",\n",
    "#     \"computing devices can be charged without being plugged in\",\n",
    "#     \"key contributor to climate change\",\n",
    "#     \"we are powered by imagination\",\n",
    "#     \"the machines we use to think\",\n",
    "#     \"the machines that are destroying the planet\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme 5 - On Beauty, Aesthetics, and the Sublime\n",
    "\n",
    "hold videos ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/r/personal/rtwomey2_unl_edu/Documents/POM_AI/pom_videos_hold.zip?csf=1&web=1&e=PF8MQN)):\n",
    "```\n",
    "pom_128_30_hold_a_painting_of_pygmalion.mp4\n",
    "pom_128_30_hold_a_sculptor_who_fell_in_love_with_his_own_sculpture.mp4\n",
    "pom_128_30_hold_everyone_who_saw_her_would_fall_in_love_with_her.mp4 \n",
    "pom_128_30_hold_beautiful_reflections_of_our_digital_twinse.mp4 \n",
    "pom_128_30_hold_beautiful_reflections_of_our_digital_twins.mp4 \n",
    "pom_128_30_hold_a_statue_of_a_woman.mp4\n",
    "pom_128_30_hold_a_living_breathing_woman.mp4\n",
    "pom_128_30_hold_our_digital_twins__artificial_intelligence.mp4 \n",
    "pom_128_30_hold_a_projection_of_our_deepest_fears.mp4\n",
    "```\n",
    "\n",
    "interpolation videos to be generated:\n",
    "- one video transitioning between each of the phrases in this list (1->2, 2->3, 3->4, n->1)\n",
    "- videos of transitions between all pairwise combinations of the list\n",
    "\n",
    "latent vectors for each phrase are in `results_pom_seed_128.tar.gz` ([onedrive](https://uofnelincoln-my.sharepoint.com/:u:/g/personal/rtwomey2_unl_edu/EVB51YvooN5Fu_0l7jQJhbkB_lx97kcyQ5oBL4Md6Dvvug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # theme 5\n",
    "#     \"a painting of pygmalion\",\n",
    "#     \"a sculptor who fell in love with his own sculpture\",\n",
    "#     \"everyone who saw her would fall in love with her\",\n",
    "#     \"beautiful reflections of our digital twinse\",\n",
    "#     \"a statue of a woman\",\n",
    "#     \"a living breathing woman\",\n",
    "#     \"our digital twins, artificial intelligence\",\n",
    "#     \"a projection of our deepest fears\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WanderCLIP.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.6 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.6-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
