{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e11d1f-b22d-4f49-a0f2-33bd64e320d0",
   "metadata": {},
   "source": [
    "Winter Break 2022 work. \n",
    "\n",
    "Run on jupyterhub-west.nrp-nautilus.io with a RTX A100 GPU.\n",
    "\n",
    "Adapted from https://colab.research.google.com/github/ouhenio/StyleGAN3-CLIP-notebook/blob/main/StyleGAN3%2Binversion%2BCLIP.ipynb#scrollTo=5K38uyFrv5wo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732e3ab2-5e84-4e2f-aa50-5aee170ba6b3",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "One time install of libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84e4fe-b4c1-48d5-b36e-b5496d9d1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install --upgrade https://download.pytorch.org/whl/nightly/cu111/torch-1.11.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cu111/torchvision-0.12.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
    "!git clone https://github.com/NVlabs/stylegan3\n",
    "!git clone https://github.com/openai/CLIP\n",
    "!pip install -e ./CLIP\n",
    "!pip install einops ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ea739-acdf-4f07-a002-9ca45183891d",
   "metadata": {},
   "source": [
    "# 1. Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db054a-c99c-46f5-ba7f-cae04be130ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./CLIP')\n",
    "sys.path.append('./stylegan3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6ac9a-e670-4620-ae6a-5ae9389c7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os, time\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import clip\n",
    "import copy\n",
    "import imageio\n",
    "import unicodedata\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from IPython.display import display\n",
    "from einops import rearrange\n",
    "# from google.colab import files\n",
    "from time import perf_counter\n",
    "from stylegan3.dnnlib.util import open_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4e49f2-9cd8-4dd4-b359-8f4c3b24f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Load VGG16 feature detector.\n",
    "url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'\n",
    "with open_url(url) as f:\n",
    "    vgg16 = torch.jit.load(f).eval().to(device)\n",
    "print('Using device:', device, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b755d9-96b8-4e9c-88f1-237c6cbda633",
   "metadata": {},
   "source": [
    "## Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245eb5f9-53a9-4832-816c-9f6262166ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "def fetch_model(url_or_path):\n",
    "    if \"drive.google\" in url_or_path:\n",
    "      if \"18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj\" in url_or_path: \n",
    "        basename = \"wikiart-1024-stylegan3-t-17.2Mimg.pkl\"\n",
    "      elif \"14UGDDOusZ9TMb-pOrF0PAjMGVWLSAii1\" in url_or_path:\n",
    "        basename = \"lhq-256-stylegan3-t-25Mimg.pkl\"\n",
    "    else:\n",
    "        basename = os.path.basename(url_or_path)\n",
    "    if os.path.exists(basename):\n",
    "        return basename\n",
    "    else:\n",
    "        if \"drive.google\" not in url_or_path:\n",
    "          !wget -c '{url_or_path}'\n",
    "        else:\n",
    "          path_id = url_or_path.split(\"id=\")[-1]\n",
    "          !gdown --id '{path_id}'\n",
    "        return basename\n",
    "\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n",
    "\n",
    "def norm1(prompt):\n",
    "    \"Normalize to the unit sphere.\"\n",
    "    return prompt / prompt.square().sum(dim=-1,keepdim=True).sqrt()\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "class MakeCutouts(torch.nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, input):\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
    "\n",
    "def embed_image(image):\n",
    "  n = image.shape[0]\n",
    "  cutouts = make_cutouts(image)\n",
    "  embeds = clip_model.embed_cutout(cutouts)\n",
    "  embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
    "  return embeds\n",
    "\n",
    "def embed_url(url):\n",
    "  image = Image.open(fetch(url)).convert('RGB')\n",
    "  return embed_image(TF.to_tensor(image).to(device).unsqueeze(0)).mean(0).squeeze(0)\n",
    "\n",
    "class CLIP(object):\n",
    "  def __init__(self):\n",
    "    clip_model = \"ViT-B/32\"\n",
    "    self.model, _ = clip.load(clip_model)\n",
    "    self.model = self.model.requires_grad_(False)\n",
    "    self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                          std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def embed_text(self, prompt):\n",
    "      \"Normalized clip text embedding.\"\n",
    "      return norm1(self.model.encode_text(clip.tokenize(prompt).to(device)).float())\n",
    "\n",
    "  def embed_cutout(self, image):\n",
    "      \"Normalized clip image embedding.\"\n",
    "      return norm1(self.model.encode_image(self.normalize(image)))\n",
    "  \n",
    "clip_model = CLIP()\n",
    "\n",
    "# Projector\n",
    "\n",
    "def project(\n",
    "    G,\n",
    "    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution\n",
    "    *,\n",
    "    num_steps                  = 1000,\n",
    "    w_avg_samples              = -1,\n",
    "    initial_learning_rate      = 0.1,\n",
    "    initial_noise_factor       = 0.05,\n",
    "    lr_rampdown_length         = 0.25,\n",
    "    lr_rampup_length           = 0.05,\n",
    "    noise_ramp_length          = 0.75,\n",
    "    regularize_noise_weight    = 1e5,\n",
    "    verbose                    = False,\n",
    "    device: torch.device\n",
    "):\n",
    "\n",
    "    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)\n",
    "\n",
    "    def logprint(*args):\n",
    "        if verbose:\n",
    "            print(*args)\n",
    "\n",
    "    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "    # Compute w stats.\n",
    "    if w_avg_samples > 0:\n",
    "      logprint(f'Computing W midpoint and stddev using {w_avg_samples} samples...')\n",
    "      z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)\n",
    "    else:\n",
    "      seed = np.random.randint(0, 2**32 - 1)\n",
    "      z_samples = np.random.RandomState(seed).randn(1, G.z_dim)\n",
    "    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]\n",
    "    w_samples = w_samples[:, :1, :].cpu().numpy().astype(np.float32)       # [N, 1, C]\n",
    "    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 1, C]\n",
    "    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5\n",
    "\n",
    "    # Setup noise inputs.\n",
    "    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }\n",
    "\n",
    "    # Features for target image.\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
    "    target_features = vgg16(target_images, resize_images=False, return_lpips=True)\n",
    "\n",
    "    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\n",
    "    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\n",
    "    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n",
    "\n",
    "    # Init noise.\n",
    "    for buf in noise_bufs.values():\n",
    "        buf[:] = torch.randn_like(buf)\n",
    "        buf.requires_grad = True\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Learning rate schedule.\n",
    "        t = step / num_steps\n",
    "        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2\n",
    "        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)\n",
    "        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n",
    "        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)\n",
    "        lr = initial_learning_rate * lr_ramp\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # Synth images from opt_w.\n",
    "        w_noise = torch.randn_like(w_opt) * w_noise_scale\n",
    "        ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n",
    "        synth_images = G.synthesis(ws, noise_mode='const')\n",
    "\n",
    "        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "        synth_images = (synth_images + 1) * (255/2)\n",
    "        if synth_images.shape[2] > 256:\n",
    "            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n",
    "\n",
    "        # Features for synth images.\n",
    "        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n",
    "        dist = (target_features - synth_features).square().sum()\n",
    "\n",
    "        # Noise regularization.\n",
    "        reg_loss = 0.0\n",
    "        for v in noise_bufs.values():\n",
    "            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n",
    "            while True:\n",
    "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n",
    "                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n",
    "                if noise.shape[2] <= 8:\n",
    "                    break\n",
    "                noise = F.avg_pool2d(noise, kernel_size=2)\n",
    "        loss = dist + reg_loss * regularize_noise_weight\n",
    "\n",
    "        # Step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logprint(f'step {step+1:>4d}/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n",
    "\n",
    "        # Save projected W for each optimization step.\n",
    "        w_out[step] = w_opt.detach()[0]\n",
    "\n",
    "        # Normalize noise.\n",
    "        with torch.no_grad():\n",
    "            for buf in noise_bufs.values():\n",
    "                buf -= buf.mean()\n",
    "                buf *= buf.square().mean().rsqrt()\n",
    "\n",
    "    return w_out.repeat([1, G.mapping.num_ws, 1])\n",
    "\n",
    "def get_perceptual_loss(synth_image, target_features):\n",
    "    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    if synth_image.shape[2] > 256:\n",
    "        synth_image = F.interpolate(synth_image, size=(256, 256), mode='area')\n",
    "\n",
    "    # Features for synth images.\n",
    "    synth_features = vgg16(synth_image, resize_images=False, return_lpips=True)\n",
    "    return (target_features - synth_features).square().sum()\n",
    "\n",
    "def get_target_features(target):\n",
    "    target_images = target.unsqueeze(0).to(device).to(torch.float32)\n",
    "    if target_images.shape[2] > 256:\n",
    "        target_images = F.interpolate(target_images, size=(256, 256), mode='area')\n",
    "    return vgg16(target_images, resize_images=False, return_lpips=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1b256-3e1c-4c9b-b036-81b42cb92e0c",
   "metadata": {},
   "source": [
    "Install gdown and use it to download model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822700d-1f96-447b-a4c5-ff1acd23339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e0ad2-2738-4bc9-a488-1fa0cd4c7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f1a676-993a-45dd-830b-4174d3854f43",
   "metadata": {},
   "source": [
    "Model selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e53819-ed08-4124-924d-022c9de8d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/\"\n",
    "\n",
    "Model = 'Wikiart' #@param [\"FFHQ\", \"MetFaces\", \"AFHQv2\", \"cosplay\", \"Wikiart\", \"Landscapes\"]\n",
    "\n",
    "model_name = {\n",
    "    \"FFHQ\": base_url + \"stylegan3-t-ffhqu-1024x1024.pkl\",\n",
    "    \"MetFaces\": base_url + \"stylegan3-r-metfacesu-1024x1024.pkl\",\n",
    "    \"AFHQv2\": base_url + \"stylegan3-t-afhqv2-512x512.pkl\",\n",
    "    \"cosplay\": \"https://l4rz.net/cosplayface-snapshot-stylegan3t-008000.pkl\",\n",
    "    \"Wikiart\": \"https://drive.google.com/u/0/open?id=18MOpwTMJsl_Z17q-wQVnaRLCUFZYSNkj\",\n",
    "    \"Landscapes\": \"https://drive.google.com/u/0/open?id=14UGDDOusZ9TMb-pOrF0PAjMGVWLSAii1\"\n",
    "}\n",
    "\n",
    "# network_url = model_name[Model]\n",
    "model_path = \"wikiart-1024-stylegan3-t-17.2Mimg.pkl\"\n",
    "# with open(fetch_model(network_url), 'rb') as fp:\n",
    "\n",
    "with open(model_path, 'rb') as fp:\n",
    "  G = pickle.load(fp)['G_ema'].to(device)\n",
    "\n",
    "zs = torch.randn([10000, G.mapping.z_dim], device=device)\n",
    "w_stds = G.mapping(zs, None).std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965887d5-1eff-41ec-a871-508d4b27c8eb",
   "metadata": {},
   "source": [
    "## Parameters for Text to Image Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8133a-fdac-44ce-9af8-25c0161d6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_filename = \"Red_Blossom_Cropped.jpg\" #@param {type:\"string\"}\n",
    "text = \"\" #@param {type:\"string\"}\n",
    "loss_ratio = 0.4#@param {type:\"number\"}\n",
    "steps = 800 #@param {type:\"number\"}\n",
    "limit_step = 600 #@param {type:\"number\"}\n",
    "seed = 34 # 14 default #@param {type:\"number\"}\n",
    "\n",
    "if seed == -1:\n",
    "    seed = np.random.randint(0,9e9)\n",
    "\n",
    "target = clip_model.embed_text(text)\n",
    "\n",
    "target_pil = Image.open(target_image_filename).convert('RGB')\n",
    "w, h = target_pil.size\n",
    "s = min(w, h)\n",
    "target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))\n",
    "target_pil = target_pil.resize((G.img_resolution, G.img_resolution), Image.LANCZOS)\n",
    "target_uint8 = np.array(target_pil, dtype=np.uint8)\n",
    "target_tensor = torch.tensor(target_uint8.transpose([2, 0, 1]), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d098f-2b16-4dfc-b961-444178242131",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e078c2-b525-4039-a0f2-b87cb4d93ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually do the run\n",
    "\n",
    "tf = Compose([\n",
    "  Resize(224),\n",
    "  lambda x: torch.clamp((x+1)/2,min=0,max=1),\n",
    "])\n",
    "\n",
    "\n",
    "def run(timestring, projection_target):\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "  target_features = get_target_features(projection_target)\n",
    "\n",
    "  # Init\n",
    "  # Sample 32 inits and choose the one closest to prompt\n",
    "\n",
    "  with torch.no_grad():\n",
    "    qs = []\n",
    "    losses = []\n",
    "    for _ in range(8):\n",
    "      q = (G.mapping(torch.randn([4,G.mapping.z_dim], device=device), None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
    "      images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
    "      loss = get_perceptual_loss(images, target_features)\n",
    "      i = torch.argmin(loss)\n",
    "      qs.append(q[i])\n",
    "      losses.append(loss)\n",
    "    qs = torch.stack(qs)\n",
    "    losses = torch.stack(losses)\n",
    "    i = torch.argmin(losses)\n",
    "    q = qs[i].unsqueeze(0).requires_grad_()\n",
    "\n",
    "  # Sampling loop\n",
    "  q_ema = q\n",
    "  opt = torch.optim.AdamW([q], lr=0.03, betas=(0.0,0.999))\n",
    "  loop = tqdm(range(steps))\n",
    "  for i in loop:\n",
    "    opt.zero_grad()\n",
    "    w = q * w_stds\n",
    "    image = G.synthesis(w + G.mapping.w_avg, noise_mode='const')\n",
    "    embed = embed_image(image.add(1).div(2))\n",
    "    step_ratio = i / limit_step\n",
    "    perceptual_loss = get_perceptual_loss(image, target_features)\n",
    "    modulated_perceptual_loss = (\n",
    "        max(loss_ratio, 1 - step_ratio)\n",
    "        * get_perceptual_loss(image, target_features)\n",
    "    )\n",
    "    clip_loss = spherical_dist_loss(embed, target).mean()\n",
    "    modulated_clip_loss = (\n",
    "        min(1 - loss_ratio, step_ratio)\n",
    "        * (step_ratio) * spherical_dist_loss(embed, target).mean()\n",
    "    )\n",
    "    loss = modulated_perceptual_loss + modulated_clip_loss\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    loop.set_postfix(loss=loss.item(), q_magnitude=q.std().item())\n",
    "\n",
    "    q_ema = q_ema * 0.9 + q * 0.1\n",
    "    image = G.synthesis(q_ema * w_stds + G.mapping.w_avg, noise_mode='const')\n",
    "\n",
    "    if i % 10 == 0:\n",
    "      display(TF.to_pil_image(tf(image)[0]))\n",
    "      print(f\"image {i}/{steps} | projector loss: {perceptual_loss} | clip loss: {clip_loss} | modulated loss: {loss}\")\n",
    "    pil_image = TF.to_pil_image(image[0].add(1).div(2).clamp(0,1))\n",
    "    os.makedirs(f'samples/{timestring}', exist_ok=True)\n",
    "    pil_image.save(f'samples/{timestring}/{i:04}.jpg')\n",
    "\n",
    "try:\n",
    "  timestring = time.strftime('%Y%m%d%H%M%S')\n",
    "  run(timestring, target_tensor)\n",
    "except KeyboardInterrupt:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a933f9-a0c1-431f-b509-0389572e5207",
   "metadata": {},
   "source": [
    "## Save files\n",
    "\n",
    "download the tar file created by this step manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b1fd1-b40d-4ccc-927b-2bf61b539eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown #**Save images** 📷\n",
    "#@markdown A `.tar` file will be saved inside *samples* and automatically downloaded, unless you previously ran the Google Drive cell,\n",
    "#@markdown in which case it'll be saved inside your previously created drive *samples* folder.\n",
    "\n",
    "archive_name = \"optional\"#@param {type:\"string\"}\n",
    "\n",
    "archive_name = slugify(archive_name)\n",
    "\n",
    "if archive_name != \"optional\":\n",
    "  fname = archive_name\n",
    "  # os.rename(f'samples/{timestring}', f'samples/{fname}')\n",
    "else:\n",
    "  fname = timestring\n",
    "# Save images as a tar archive\n",
    "!tar cf samples/{fname}.tar samples/{timestring}\n",
    "\n",
    "## requires the google files extension\n",
    "# if os.path.isdir('drive/MyDrive/samples'):\n",
    "#   shutil.copyfile(f'samples/{fname}.tar', f'drive/MyDrive/samples/{fname}.tar')\n",
    "# else:\n",
    "#   files.download(f'samples/{fname}.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510aed0a-87a1-4b51-b093-a246a52b97bd",
   "metadata": {},
   "source": [
    "## Make a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d18136-398a-4718-8bef-1d0cac544afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown You can edit frame rate and stuff by double-clicking this tab.\n",
    "\n",
    "frames = os.listdir(f\"samples/{timestring}\")\n",
    "frames = len(list(filter(lambda filename: filename.endswith(\".jpg\"), frames))) #Get number of jpg generated\n",
    "\n",
    "init_frame = 1 #This is the frame where the video will start\n",
    "last_frame = frames #You can change i to the number of the last frame you want to generate. It will raise an error if that number of frames does not exist.\n",
    "\n",
    "min_fps = 10\n",
    "max_fps = 60\n",
    "\n",
    "total_frames = last_frame-init_frame\n",
    "\n",
    "#Desired video time in seconds\n",
    "video_length = 14 #@param {type:\"number\"}\n",
    "#Video filename\n",
    "video_name = \"\" #@param {type:\"string\"}\n",
    "video_name = target_image_filename\n",
    "video_name = slugify(video_name)\n",
    "\n",
    "# frames = []\n",
    "# tqdm.write('Generating video...')\n",
    "# for i in range(init_frame,last_frame): #\n",
    "#     filename = f\"samples/{timestring}/{i:04}.jpg\"\n",
    "#     frames.append(Image.open(filename))\n",
    "\n",
    "fps = np.clip(total_frames/video_length,min_fps,max_fps)\n",
    "\n",
    "!ffmpeg -r {fps} -i samples/{timestring}/%04d.jpg -c:v libx264 -vf fps={fps} -pix_fmt yuv420p samples/{video_name}.mp4 -frames:v {total_frames}\n",
    "\n",
    "# from subprocess import Popen, PIPE\n",
    "# p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', f'samples/{video_name}.mp4'], stdin=PIPE)\n",
    "# for im in tqdm(frames):\n",
    "#     im.save(p.stdin, 'PNG')\n",
    "# p.stdin.close()\n",
    "\n",
    "# print(\"The video is now being compressed, wait...\")\n",
    "# p.wait()\n",
    "# print(\"The video is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f6df1-5d4b-42c2-a94e-724ca73fc97a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
